{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "130c8556",
   "metadata": {},
   "source": [
    "# White Blood Cell Image Classification\n",
    "### By [Anthony Medina](https://www.linkedin.com/in/anthony-medina-math/)\n",
    "\n",
    "# Modeling Notebook\n",
    "1. Notebook Objectives\n",
    "2. Imports\n",
    "3. Final Pre-Building Checks\n",
    "4. Model 1 Neural Network\n",
    "5. Model 2 Random Forest\n",
    "6. Model 3 Gradient Boosting Machine\n",
    "7. Model results analysis\n",
    "8. Model Choice\n",
    "9. Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7ab778",
   "metadata": {},
   "source": [
    "### 1. Notebook Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b07f8c1",
   "metadata": {},
   "source": [
    "This notebook will house the model building, evaluation of each model, and picking the model with best Recall score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcaa299",
   "metadata": {},
   "source": [
    "### 2. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1bfb6a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import matplotlib.image as mpimg\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import make_scorer, recall_score, confusion_matrix\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf1c9cb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cell_name      object\n",
       "image_array    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ab40c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 neutrophil\n",
      "1 monocyte\n",
      "2 lymphocyte\n",
      "3 eosinophil\n"
     ]
    }
   ],
   "source": [
    "# I added this block because importing my clean data was a nightmare.\n",
    "# New Array that will contain the final values I need to save for modeling.\n",
    "#import cv2\n",
    "\n",
    "column_names = ['cell_name', 'image_array']\n",
    "\n",
    "# Create a blank DataFrame with column names\n",
    "df = pd.DataFrame(columns=column_names)\n",
    "\n",
    "# Populating the data frame from the 4 different types of images\n",
    "cell_names = ['neutrophil', 'monocyte', 'lymphocyte', 'eosinophil']\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "for index, cell_name in enumerate(cell_names):\n",
    "    print(index, cell_name)\n",
    "    directory_path = '../raw_data/organized_data_set/images/' + cell_name\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if os.path.isfile(os.path.join(directory_path, filename)):\n",
    "            file_path = os.path.abspath(os.path.join(directory_path, filename))\n",
    "            image = mpimg.imread(file_path) # First it's an image\n",
    "            first_array = np.array(image) # Then it's an array\n",
    "            float_array = first_array.astype('float32') # Now it's an array of floats\n",
    "            rescaled_array = float_array / 255.0 # Rescaling the float\n",
    "            new_entry = {\"cell_name\": cell_name, 'image_array': rescaled_array}\n",
    "            df.loc[len(df)] = new_entry\n",
    "            # images.append(image)\n",
    "            images.append(rescaled_array)\n",
    "            labels.append(cell_name)\n",
    "\n",
    "                \n",
    "X = np.array(images)\n",
    "y = np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cd8aa5",
   "metadata": {},
   "source": [
    "# GET RID OF THIS\n",
    "# This is the small version of the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "498b9f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 neutrophil\n",
      "1 monocyte\n",
      "2 lymphocyte\n",
      "3 eosinophil\n"
     ]
    }
   ],
   "source": [
    "# I added this block because importing my clean data was a nightmare.\n",
    "# New Array that will contain the final values I need to save for modeling.\n",
    "#import cv2\n",
    "\n",
    "column_names = ['cell_name', 'image_array']\n",
    "\n",
    "# Create a blank DataFrame with column names\n",
    "df = pd.DataFrame(columns=column_names)\n",
    "\n",
    "# Populating the data frame from the 4 different types of images\n",
    "cell_names = ['neutrophil', 'monocyte', 'lymphocyte', 'eosinophil']\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "for index, cell_name in enumerate(cell_names):\n",
    "    print(index, cell_name)\n",
    "    directory_path = '../raw_data/organized_data_set/images/' + cell_name\n",
    "    count = 0\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if count < 150:\n",
    "            count +=1\n",
    "            if os.path.isfile(os.path.join(directory_path, filename)):\n",
    "                file_path = os.path.abspath(os.path.join(directory_path, filename))\n",
    "    #            print(file_path)\n",
    "                image = mpimg.imread(file_path) # First it's an image\n",
    "    #            image = cv2.imread(file_path)\n",
    "                first_array = np.array(image) # Then it's an array\n",
    "    #            reshaped_array = first_array.flatten() # Now it's a flat array\n",
    "                float_array = first_array.astype('float32') # Now it's an array of floats\n",
    "                rescaled_array = float_array / 255.0 # Rescaling the float\n",
    "                new_entry = {\"cell_name\": cell_name, 'image_array': rescaled_array}\n",
    "                df.loc[len(df)] = new_entry\n",
    "                # images.append(image)\n",
    "                images.append(rescaled_array)\n",
    "                labels.append(cell_name)\n",
    "                \n",
    "                \n",
    "X = np.array(images)\n",
    "y = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "d4335886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600 600\n"
     ]
    }
   ],
   "source": [
    "print(len(X), len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aaaeb1f",
   "metadata": {},
   "source": [
    "### 3. Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "717a67ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "y_encoded = encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "4e499d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n"
     ]
    }
   ],
   "source": [
    "print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "0ec04084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "2dd54c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600 4\n"
     ]
    }
   ],
   "source": [
    "print(len(y_encoded), len(set(y_encoded)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "60e5f880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 320, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_train[12].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d290bd42",
   "metadata": {},
   "source": [
    "# 1. Efficient Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "5bd6e4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "12/12 [==============================] - 9s 698ms/step - loss: 44.1896 - accuracy: 0.2161 - val_loss: 7.8223 - val_accuracy: 0.2917\n",
      "Epoch 2/10\n",
      "12/12 [==============================] - 8s 687ms/step - loss: 6.1403 - accuracy: 0.2786 - val_loss: 4.0328 - val_accuracy: 0.2917\n",
      "Epoch 3/10\n",
      "12/12 [==============================] - 9s 728ms/step - loss: 2.1633 - accuracy: 0.2943 - val_loss: 2.0064 - val_accuracy: 0.2083\n",
      "Epoch 4/10\n",
      "12/12 [==============================] - 8s 699ms/step - loss: 1.0896 - accuracy: 0.5677 - val_loss: 1.4960 - val_accuracy: 0.2812\n",
      "Epoch 5/10\n",
      "12/12 [==============================] - 8s 689ms/step - loss: 0.8012 - accuracy: 0.7812 - val_loss: 1.6534 - val_accuracy: 0.3021\n",
      "Epoch 6/10\n",
      "12/12 [==============================] - 8s 690ms/step - loss: 0.5517 - accuracy: 0.9141 - val_loss: 1.5683 - val_accuracy: 0.2604\n",
      "Epoch 7/10\n",
      "12/12 [==============================] - 9s 727ms/step - loss: 0.3482 - accuracy: 0.9740 - val_loss: 1.6702 - val_accuracy: 0.2812\n",
      "Epoch 8/10\n",
      "12/12 [==============================] - 8s 691ms/step - loss: 0.2367 - accuracy: 0.9922 - val_loss: 1.8057 - val_accuracy: 0.3125\n",
      "Epoch 9/10\n",
      "12/12 [==============================] - 8s 699ms/step - loss: 0.1415 - accuracy: 0.9974 - val_loss: 1.8304 - val_accuracy: 0.3229\n",
      "Epoch 10/10\n",
      "12/12 [==============================] - 8s 708ms/step - loss: 0.0848 - accuracy: 1.0000 - val_loss: 1.8827 - val_accuracy: 0.3021\n",
      "4/4 [==============================] - 1s 156ms/step\n",
      "Recall: 0.3136998059411853\n",
      "[[13  8  4  5]\n",
      " [ 8 12  3  5]\n",
      " [ 8  9  7  5]\n",
      " [14 10  4  5]]\n"
     ]
    }
   ],
   "source": [
    "# Efficient Net\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "\n",
    "# Assuming you have image data and labels (X and y)\n",
    "\n",
    "# Load the pre-trained EfficientNetB0 model\n",
    "base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(240, 320, 3))\n",
    "\n",
    "# Build your custom head on top of the base model\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(4, activation='softmax')  # Assuming 4 classes\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model = create_model()\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "recall = recall_score(y_test, y_pred_classes, average='macro')\n",
    "print(f'Recall: {recall}')\n",
    "\n",
    "confusion_mtx = confusion_matrix(y_test, y_pred_classes)\n",
    "\n",
    "print(confusion_mtx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9d76d6",
   "metadata": {},
   "source": [
    "### 4. Model 1 Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "d6082a38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "12/12 [==============================] - 9s 676ms/step - loss: 26.5027 - accuracy: 0.2943 - val_loss: 6.2154 - val_accuracy: 0.2604\n",
      "Epoch 2/10\n",
      "12/12 [==============================] - 8s 669ms/step - loss: 2.6625 - accuracy: 0.3307 - val_loss: 1.5159 - val_accuracy: 0.2812\n",
      "Epoch 3/10\n",
      "12/12 [==============================] - 8s 663ms/step - loss: 0.9246 - accuracy: 0.6510 - val_loss: 1.4509 - val_accuracy: 0.3229\n",
      "Epoch 4/10\n",
      "12/12 [==============================] - 8s 682ms/step - loss: 0.5915 - accuracy: 0.8333 - val_loss: 1.6537 - val_accuracy: 0.2812\n",
      "Epoch 5/10\n",
      "12/12 [==============================] - 8s 676ms/step - loss: 0.2581 - accuracy: 0.9661 - val_loss: 1.7286 - val_accuracy: 0.2292\n",
      "Epoch 6/10\n",
      "12/12 [==============================] - 8s 684ms/step - loss: 0.1269 - accuracy: 1.0000 - val_loss: 1.9266 - val_accuracy: 0.2604\n",
      "Epoch 7/10\n",
      "12/12 [==============================] - 8s 654ms/step - loss: 0.0665 - accuracy: 1.0000 - val_loss: 2.0481 - val_accuracy: 0.2500\n",
      "Epoch 8/10\n",
      "12/12 [==============================] - 8s 688ms/step - loss: 0.0376 - accuracy: 1.0000 - val_loss: 2.1272 - val_accuracy: 0.2500\n",
      "Epoch 9/10\n",
      "12/12 [==============================] - 8s 668ms/step - loss: 0.0214 - accuracy: 1.0000 - val_loss: 2.2914 - val_accuracy: 0.2500\n",
      "Epoch 10/10\n",
      "12/12 [==============================] - 9s 713ms/step - loss: 0.0130 - accuracy: 1.0000 - val_loss: 2.3611 - val_accuracy: 0.2812\n",
      "4/4 [==============================] - 1s 148ms/step\n",
      "Recall: 0.32344939543215406\n",
      "[[ 8  6 11  5]\n",
      " [ 8 11  5  4]\n",
      " [ 3  7 14  5]\n",
      " [ 8  7 13  5]]\n"
     ]
    }
   ],
   "source": [
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(240, 320, 3)))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(len(set(y_encoded)), activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss= 'sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "recall = recall_score(y_test, y_pred_classes, average='macro')\n",
    "print(f'Recall: {recall}')\n",
    "\n",
    "confusion_mtx = confusion_matrix(y_test, y_pred_classes)\n",
    "\n",
    "print(confusion_mtx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd42ae2",
   "metadata": {},
   "source": [
    "### Best Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c83c1b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9c0e110",
   "metadata": {},
   "source": [
    "# Flattening the images for random forest and svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b5e9154c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 240, 320, 3)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "f1f72261",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples, height, width, channels = X.shape\n",
    "X_flat = X.reshape(num_samples, -1)\n",
    "\n",
    "# flattened_images is now a 2D array with shape (num_samples, height * width) or (num_samples, height * width * channels) for color images\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_flat, y_encoded, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "78a8bfbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 230400)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_flat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecae0208",
   "metadata": {},
   "source": [
    "### 5. Model 2 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "2ca00e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.38666666666666666\n",
      "Confusion Matrix:\n",
      "[[55 34 22 39]\n",
      " [37 71 17 25]\n",
      " [24 18 78 30]\n",
      " [36 35 51 28]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_predict, train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Assuming you have features X and corresponding labels y\n",
    "\n",
    "# Create the Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Perform cross-validation and get predicted labels\n",
    "y_pred_cv = cross_val_predict(rf_classifier, X_flat, y_encoded, cv=5)\n",
    "\n",
    "# Calculate and print the confusion matrix\n",
    "recall = recall_score(y_encoded, y_pred_cv, average='macro')\n",
    "print(f'Recall: {recall}')\n",
    "\n",
    "conf_matrix = confusion_matrix(y_encoded, y_pred_cv)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9de4a285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[41  3  0  6]\n",
      " [ 3 44  2  1]\n",
      " [ 7  2 39  2]\n",
      " [ 1  1  1 47]]\n",
      "Best Parameters: {'max_depth': None, 'min_samples_split': 8, 'n_estimators': 50}\n",
      "Recall: 0.275\n"
     ]
    }
   ],
   "source": [
    "\"\"\"# Assuming you have X_train, y_train for your data\n",
    "# You should also have X_test and y_test for evaluation\n",
    "\n",
    "# Define the Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier()\n",
    "\n",
    "# Define hyperparameters and their possible values for grid search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 5, 10],\n",
    "    'min_samples_split': [4, 8, 10]\n",
    "}\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "recall_scorer = make_scorer(recall_score, average='macro')\n",
    "grid_search = GridSearchCV(rf_classifier, param_grid, cv=3, scoring=recall_scorer)\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and best estimator\n",
    "best_params = grid_search.best_params_\n",
    "best_estimator = grid_search.best_estimator_\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Calculate confusion matrix\n",
    "y_pred = best_estimator.predict(X_flat)\n",
    "conf_matrix = confusion_matrix(y_encoded, y_pred)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)\n",
    "\n",
    "# Evaluate the model\n",
    "recall = best_estimator.score(X_test, y_test)\n",
    "print(f'Best Parameters: {best_params}')\n",
    "print(f'Recall: {recall}')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5ac71e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[41  3  0  6]\n",
      " [ 3 44  2  1]\n",
      " [ 7  2 39  2]\n",
      " [ 1  1  1 47]]\n",
      "Best Parameters: {'max_depth': None, 'min_samples_split': 8, 'n_estimators': 50}\n",
      "Recall: 0.275\n"
     ]
    }
   ],
   "source": [
    "\"\"\"# Calculate confusion matrix\n",
    "y_pred = best_estimator.predict(X_flat)\n",
    "conf_matrix = confusion_matrix(y_encoded, y_pred)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)\n",
    "\n",
    "# Evaluate the model\n",
    "recall = best_estimator.score(X_test, y_test)\n",
    "print(f'Best Parameters: {best_params}')\n",
    "print(f'Recall: {recall}')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b310d82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "edd17736",
   "metadata": {},
   "source": [
    "### Best Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734d1484",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0b79cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a813b766",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a09d3f0",
   "metadata": {},
   "source": [
    "### 6. Model 3 Gradient Boosting Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2532fc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "52a4e60d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[206], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m gb_classifier \u001b[38;5;241m=\u001b[39m GradientBoostingClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Train the classifier on the training data\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mgb_classifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Predict the labels for the test set\u001b[39;00m\n\u001b[1;32m      7\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m gb_classifier\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_gb.py:538\u001b[0m, in \u001b[0;36mBaseGradientBoosting.fit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resize_state()\n\u001b[1;32m    537\u001b[0m \u001b[38;5;66;03m# fit the boosting stages\u001b[39;00m\n\u001b[0;32m--> 538\u001b[0m n_stages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_stages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    542\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbegin_at_stage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;66;03m# change shape of arrays after fit (early-stopping or additional ests)\u001b[39;00m\n\u001b[1;32m    552\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_stages \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_gb.py:615\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stages\u001b[0;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001b[0m\n\u001b[1;32m    608\u001b[0m     old_oob_score \u001b[38;5;241m=\u001b[39m loss_(\n\u001b[1;32m    609\u001b[0m         y[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[1;32m    610\u001b[0m         raw_predictions[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[1;32m    611\u001b[0m         sample_weight[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[1;32m    612\u001b[0m     )\n\u001b[1;32m    614\u001b[0m \u001b[38;5;66;03m# fit next stage of trees\u001b[39;00m\n\u001b[0;32m--> 615\u001b[0m raw_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_stage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m    \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_csc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_csr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;66;03m# track deviance (= loss)\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_oob:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_gb.py:257\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stage\u001b[0;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001b[0m\n\u001b[1;32m    254\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m sample_weight \u001b[38;5;241m*\u001b[39m sample_mask\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[1;32m    256\u001b[0m X \u001b[38;5;241m=\u001b[39m X_csr \u001b[38;5;28;01mif\u001b[39;00m X_csr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m X\n\u001b[0;32m--> 257\u001b[0m \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# update tree leaves\u001b[39;00m\n\u001b[1;32m    260\u001b[0m loss\u001b[38;5;241m.\u001b[39mupdate_terminal_regions(\n\u001b[1;32m    261\u001b[0m     tree\u001b[38;5;241m.\u001b[39mtree_,\n\u001b[1;32m    262\u001b[0m     X,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    269\u001b[0m     k\u001b[38;5;241m=\u001b[39mk,\n\u001b[1;32m    270\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py:1247\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m   1218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m   1219\u001b[0m     \u001b[38;5;124;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[1;32m   1220\u001b[0m \n\u001b[1;32m   1221\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1244\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m   1245\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1247\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1249\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1250\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1252\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py:379\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[1;32m    370\u001b[0m         splitter,\n\u001b[1;32m    371\u001b[0m         min_samples_split,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[1;32m    377\u001b[0m     )\n\u001b[0;32m--> 379\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    382\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gb_classifier = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "gb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = gb_classifier.predict(X_test)\n",
    "\n",
    "recall = recall_score(y_encoded, y_pred, average='macro')\n",
    "print(f'Recall: {recall}')\n",
    "\n",
    "conf_matrix = confusion_matrix(y_encoded, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "95d66bdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from sklearn.ensemble import GradientBoostingClassifier\\nfrom sklearn.model_selection import GridSearchCV\\n\\n# Assuming you have X_train, y_train for your data\\n# You should also have X_test and y_test for evaluation\\n\\n# Define the Gradient Boosting Classifier\\ngb_classifier = GradientBoostingClassifier()\\n\\n# Define hyperparameters and their possible values for grid search\\nparam_grid = {\\n    'n_estimators': [50, 100, 200],\\n    'learning_rate': [0.01, 0.1, 0.2],\\n    'max_depth': [3, 4, 5],\\n    'min_samples_split': [2, 5, 10]\\n}\\n\\n# Create a GridSearchCV object\\n\\nrecall_scorer = make_scorer(recall_score, average='macro')\\ngrid_search = GridSearchCV(gb_classifier, param_grid, cv=3, scoring=recall_scorer)\\n\\n\\n\\n# Fit the grid search to the training data\\ngrid_search.fit(X_train, y_train)\\n\\n# Get the best parameters and best estimator\\nbest_params = grid_search.best_params_\\nbest_estimator = grid_search.best_estimator_\\n\\n\\n\\n# Calculate confusion matrix\\ny_pred = best_estimator.predict(X_flat)\\nconf_matrix = confusion_matrix(y_encoded, y_pred)\\n\\n# Print the confusion matrix\\nprint('Confusion Matrix:')\\nprint(conf_matrix)\\n\\n# Evaluate the model\\nrecall = best_estimator.score(X_test, y_test)\\nprint(f'Best Parameters: {best_params}')\\nprint(f'Recall: {recall}')\""
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Assuming you have X_train, y_train for your data\n",
    "# You should also have X_test and y_test for evaluation\n",
    "\n",
    "# Define the Gradient Boosting Classifier\n",
    "gb_classifier = GradientBoostingClassifier()\n",
    "\n",
    "# Define hyperparameters and their possible values for grid search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "\n",
    "recall_scorer = make_scorer(recall_score, average='macro')\n",
    "grid_search = GridSearchCV(gb_classifier, param_grid, cv=3, scoring=recall_scorer)\n",
    "\n",
    "\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and best estimator\n",
    "best_params = grid_search.best_params_\n",
    "best_estimator = grid_search.best_estimator_\n",
    "\n",
    "\n",
    "\n",
    "# Calculate confusion matrix\n",
    "y_pred = best_estimator.predict(X_flat)\n",
    "conf_matrix = confusion_matrix(y_encoded, y_pred)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)\n",
    "\n",
    "# Evaluate the model\n",
    "recall = best_estimator.score(X_test, y_test)\n",
    "print(f'Best Parameters: {best_params}')\n",
    "print(f'Recall: {recall}')\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6d712e",
   "metadata": {},
   "source": [
    "# Without flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e7faa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2977b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Assuming you have X_train, y_train for your data\n",
    "# You should also have X_test and y_test for evaluation\n",
    "\n",
    "# Define the Gradient Boosting Classifier\n",
    "gb_classifier = GradientBoostingClassifier()\n",
    "\n",
    "# Define hyperparameters and their possible values for grid search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "\n",
    "recall_scorer = make_scorer(recall_score, average='macro')\n",
    "grid_search = GridSearchCV(gb_classifier, param_grid, cv=3, scoring=recall_scorer)\n",
    "\n",
    "\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and best estimator\n",
    "best_params = grid_search.best_params_\n",
    "best_estimator = grid_search.best_estimator_\n",
    "\n",
    "\n",
    "\n",
    "# Calculate confusion matrix\n",
    "y_pred = best_estimator.predict(X_flat)\n",
    "conf_matrix = confusion_matrix(y_encoded, y_pred)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)\n",
    "\n",
    "# Evaluate the model\n",
    "recall = best_estimator.score(X_test, y_test)\n",
    "print(f'Best Parameters: {best_params}')\n",
    "print(f'Recall: {recall}')\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58566642",
   "metadata": {},
   "source": [
    "### Best Gradient Boosting Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5300cd9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd08240b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Using Grid Search for optimizing the Neural Network\n",
    "def create_model(learning_rate=0.001, optimizer='adam', batch_size=32):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(240, 320, 3)))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(len(set(y_encoded)), activation='softmax'))  # Number of classes\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Create a KerasClassifier based on your model\n",
    "model = KerasClassifier(build_fn=create_model, epochs=5, batch_size=32, verbose=0)\n",
    "\n",
    "# Define the hyperparameters to search through\n",
    "param_grid = {\n",
    "    'learning_rate' : [0.001, 0.01],\n",
    "    'batch_size': [16, 32],\n",
    "    'epochs' : [5, 10],\n",
    "    'optimizer': ['adam', 'rmsprop']\n",
    "}\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, scoring='recall_macro')\n",
    "\n",
    "# Fit the grid search to your data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and best estimator\n",
    "best_params = grid_search.best_params_\n",
    "best_estimator = grid_search.best_estimator_\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Calculate confusion matrix\n",
    "y_pred = best_estimator.predict(X)\n",
    "conf_matrix = confusion_matrix(y_encoded, y_pred)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)\n",
    "\n",
    "# Evaluate the model\n",
    "recall = best_estimator.score(X_test, y_test)\n",
    "print(f'Best Parameters: {best_params}')\n",
    "print(f'Recall: {recall}')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf445ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"y_pred = model.predict(X)\n",
    "# Compute the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
